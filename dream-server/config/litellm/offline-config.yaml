# LiteLLM Offline Mode Configuration
# Local models only - no cloud access

model_list:
  # Local vLLM
  - model_name: qwen-32b
    litellm_params:
      model: openai/Qwen/Qwen2.5-32B-Instruct-AWQ
      api_base: http://vllm:8000/v1
      api_key: not-needed
    model_info:
      description: "Local Qwen 32B via vLLM"

  # Local Ollama (CPU fallback)
  - model_name: qwen-cpu
    litellm_params:
      model: ollama/qwen2.5:32b
      api_base: http://ollama:11434
    model_info:
      description: "Local Qwen 32B via Ollama (CPU)"

  # Default route to vLLM
  - model_name: default
    litellm_params:
      model: openai/Qwen/Qwen2.5-32B-Instruct-AWQ
      api_base: http://vllm:8000/v1
      api_key: not-needed
    model_info:
      description: "Default to local vLLM"

litellm_settings:
  drop_params: true
  set_verbose: false
  
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
