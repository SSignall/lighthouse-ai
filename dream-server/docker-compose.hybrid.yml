# docker-compose.hybrid.yml — Hybrid Mode (Local-First + Cloud Fallback)
# Mission: M1 (Fully Local OpenClaw) → M5 (Clonable Dream Setup Server)
# Mode: Local primary with cloud fallback on failure
#
# ╔══════════════════════════════════════════════════════════════════════════╗
# ║  ⚠️  SECURITY WARNING: HOST NETWORK MODE                                 ║
# ╠══════════════════════════════════════════════════════════════════════════╣
# ║  This file uses network_mode: host for simplicity. Implications:        ║
# ║                                                                          ║
# ║  • ALL services exposed on ALL host network interfaces (LAN-visible)    ║
# ║  • Open WebUI has NO authentication by default                          ║
# ║  • n8n workflows accessible without auth                                ║
# ║  • Anyone on your network can access your AI server                     ║
# ║                                                                          ║
# ║  MITIGATIONS (choose at least one):                                     ║
# ║  1. Firewall: sudo ufw deny from any to any port 4000,8000,5678,8080    ║
# ║  2. Use docker-compose.yml (bridge mode) instead for multi-user setups  ║
# ║  3. Only use on isolated/trusted networks                               ║
# ╚══════════════════════════════════════════════════════════════════════════╝

x-common-env: &common-env
  HF_HOME: /data/huggingface
  TRANSFORMERS_CACHE: /data/huggingface
  VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"

x-cloud-env: &cloud-env
  - LITELLM_MODE=hybrid
  - LOCAL_BASE_URL=http://localhost:8000/v1
  - CLOUD_API_KEY=${CLOUD_API_KEY:-}
  - CLOUD_BASE_URL=${CLOUD_BASE_URL:-https://api.openai.com/v1}

services:
  # ============================================================================
  # LiteLLM Proxy (Hybrid Router: Local → Cloud Fallback)
  # ============================================================================
  litellm:
    image: ghcr.io/berriai/litellm:v1.81.3-stable  # Stable tag format
    container_name: dream-litellm-hybrid
    restart: unless-stopped
    network_mode: host
    environment:
      - PORT=4000
      - LITELLM_MODE=hybrid
      - LOCAL_BASE_URL=http://localhost:8000/v1
      - LOCAL_MODEL=qwen2.5-32b-instruct-awq
      # Cloud fallback API keys (optional - only used on local failure)
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - CLOUD_BASE_URL=${CLOUD_BASE_URL:-https://api.openai.com/v1}
      - DEFAULT_MODEL=local
    volumes:
      - ./config/litellm/hybrid-config.yaml:/app/config.yaml:ro

  # ============================================================================
  # Local LLM Inference (vLLM) - Primary
  # ============================================================================
  vllm:
    image: vllm/vllm-openai:v0.15.1  # CVE-2024-12224 fixed in v0.15.1
    container_name: dream-vllm-hybrid
    restart: unless-stopped
    network_mode: host
    environment:
      <<: *common-env
      CUDA_VISIBLE_DEVICES: "0"
    command: >
      --model Qwen/Qwen2.5-32B-Instruct-AWQ
      ${VLLM_QUANTIZATION:+--quantization $VLLM_QUANTIZATION}
      --max-model-len ${VLLM_MAX_MODEL_LEN:-8192}
      --gpu-memory-utilization 0.92
      --enforce-eager
      --disable-custom-kernels
      --host 0.0.0.0
      --port 8000
      --enable-auto-tool-choice
      --tool-call-parser qwen3_coder
    volumes:
      - ${DREAM_DATA_DIR:-./data}:/data
      - ${DREAM_MODELS_DIR:-./models}:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

  # ============================================================================
  # Whisper Speech-to-Text (STT)
  # ============================================================================
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest
    container_name: dream-whisper-hybrid
    restart: unless-stopped
    network_mode: host
    environment:
      - DEVICE=cpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

  # ============================================================================
  # Kokoro Text-to-Speech (TTS)
  # ============================================================================
  tts:
    image: ghcr.io/remsky/kokoro-fastapi:v0.6.2-gpu
    container_name: dream-tts-hybrid
    restart: unless-stopped
    ports:
      - "${TTS_PORT:-8880}:8880"
    environment:
      - DEVICE=cuda
      - MAX_WORKERS=4
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

  # ============================================================================
  # Open WebUI (Chat Interface)
  # ============================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: dream-webui-hybrid
    restart: unless-stopped
    network_mode: host
    environment:
      - OPENAI_API_BASE_URL=http://localhost:4000/v1
      - OPENAI_API_KEY=not-needed
      - WEBUI_NAME=Dream Server (Hybrid Mode)
    volumes:
      - ./config/open-webui:/app/backend/data

  # ============================================================================
  # n8n Workflow Automation
  # ============================================================================
  n8n:
    image: n8nio/n8n:latest
    container_name: dream-n8n-hybrid
    restart: unless-stopped
    network_mode: host
    environment:
      - N8N_HOST=localhost
      - N8N_PORT=5678
    volumes:
      - ./config/n8n:/home/node/.n8n
