# Android Framework - Configuration
# https://github.com/Light-Heart-Labs/Android-Framework

# ─────────────────────────────────────────────
# Session Cleanup Settings
# ─────────────────────────────────────────────
session_cleanup:
  # Enable automatic session cleanup
  enabled: true

  # Path to your OpenClaw data directory
  # Default: ~/.openclaw
  openclaw_dir: "~/.openclaw"

  # Agent path relative to openclaw_dir
  # Default: agents/main/sessions
  sessions_path: "agents/main/sessions"

  # Maximum session file size in bytes before it gets nuked
  # When a session exceeds this, the file is deleted and its reference
  # removed from sessions.json, forcing a fresh session on next message.
  #
  # Recommended values by model context window:
  #   8K context  → 64000   (64KB)
  #   16K context → 128000  (128KB)
  #   32K context → 256000  (250KB)  ← default
  #   64K context → 512000  (500KB)
  #   128K context → 1024000 (1MB)
  max_session_size: 256000

  # How often to run cleanup (in minutes)
  # Recommended: context_window / 500 (e.g., 32K → 60 min)
  interval_minutes: 60

  # Delay after boot before first cleanup run
  boot_delay_minutes: 5

# ─────────────────────────────────────────────
# vLLM Tool Call Proxy Settings
# ─────────────────────────────────────────────
tool_proxy:
  # Enable the vLLM tool call proxy
  # Handles SSE re-wrapping, tool call extraction, response cleaning,
  # and loop protection for local models
  enabled: true

  # Port for the proxy to listen on
  port: 8003

  # Bind address (0.0.0.0 = all interfaces, 127.0.0.1 = localhost only)
  host: "0.0.0.0"

  # Your vLLM server URL
  vllm_url: "http://localhost:8000"

  # Log file location
  log_file: "~/vllm-proxy.log"

  # Maximum tool calls per conversation before aborting (loop protection)
  max_tool_calls: 500

# ─────────────────────────────────────────────
# Token Spy — API Cost & Usage Monitor
# ─────────────────────────────────────────────
token_spy:
  # Enable Token Spy API monitoring proxy
  # Sits between your agents and upstream LLM APIs, logging every request
  enabled: false

  # Agent name shown in the dashboard
  agent_name: "my-agent"

  # Port for the Token Spy proxy
  port: 9110

  # Bind address (0.0.0.0 = all interfaces, 127.0.0.1 = localhost only)
  host: "0.0.0.0"

  # Upstream API URLs — Token Spy routes by endpoint:
  #   /v1/messages         -> anthropic_upstream
  #   /v1/chat/completions -> openai_upstream
  anthropic_upstream: "https://api.anthropic.com"
  openai_upstream: ""

  # Provider type for cost calculation: anthropic, openai, moonshot, local
  api_provider: "anthropic"

  # Database backend: sqlite (zero-config) or postgres
  db_backend: "sqlite"

  # Session character limit for auto-reset (~4 chars per token)
  # Token Spy reads actual request payload sizes — smarter than file-size checks
  session_char_limit: 200000

  # Path to OpenClaw session dirs (JSON map, for session auto-reset)
  # Example: '{"my-agent":"~/.openclaw/agents/main/sessions"}'
  agent_session_dirs: ""

  # Agents using local/self-hosted models (comma-separated, get $0 cost badge)
  local_model_agents: ""

# ─────────────────────────────────────────────
# System User
# ─────────────────────────────────────────────
# The Linux user that runs OpenClaw (used for systemd services)
# install.sh will auto-detect this, but you can override it here
system_user: ""
